{
  "agents": [
    {
      "name": "test-export-agent",
      "memory_blocks": [],
      "tools": [],
      "tool_ids": [
        "tool-0",
        "tool-1",
        "tool-2",
        "tool-3",
        "tool-4"
      ],
      "source_ids": [],
      "block_ids": [],
      "tool_rules": [
        {
          "tool_name": "memory_replace",
          "type": "continue_loop",
          "prompt_template": "<tool_rule>\n{{ tool_name }} requires continuing your response when called\n</tool_rule>"
        },
        {
          "tool_name": "memory_insert",
          "type": "continue_loop",
          "prompt_template": "<tool_rule>\n{{ tool_name }} requires continuing your response when called\n</tool_rule>"
        },
        {
          "tool_name": "conversation_search",
          "type": "continue_loop",
          "prompt_template": "<tool_rule>\n{{ tool_name }} requires continuing your response when called\n</tool_rule>"
        },
        {
          "tool_name": "send_message",
          "type": "exit_loop",
          "prompt_template": "<tool_rule>\n{{ tool_name }} ends your response (yields control) when called\n</tool_rule>"
        }
      ],
      "tags": [],
      "system": "you are an agent",
      "agent_type": "memgpt_v2_agent",
      "llm_config": {
        "model": "gpt-4o-mini",
        "model_endpoint_type": "openai",
        "model_endpoint": "https://api.openai.com/v1",
        "provider_name": "openai",
        "provider_category": "base",
        "model_wrapper": null,
        "context_window": 32000,
        "put_inner_thoughts_in_kwargs": true,
        "handle": "openai/gpt-4o-mini",
        "temperature": 0.7,
        "max_tokens": null,
        "enable_reasoner": true,
        "reasoning_effort": null,
        "max_reasoning_tokens": 0,
        "frequency_penalty": 1.0,
        "compatibility_type": null,
        "verbosity": null,
        "tier": null
      },
      "embedding_config": {
        "embedding_endpoint_type": "openai",
        "embedding_endpoint": "https://api.openai.com/v1",
        "embedding_model": "text-embedding-3-small",
        "embedding_dim": 2000,
        "embedding_chunk_size": 300,
        "handle": "openai/text-embedding-3-small",
        "batch_size": 1024,
        "azure_endpoint": null,
        "azure_version": null,
        "azure_deployment": null
      },
      "initial_message_sequence": null,
      "include_base_tools": false,
      "include_multi_agent_tools": false,
      "include_base_tool_rules": false,
      "include_default_source": false,
      "description": "A blank slate for you to create your own agent from scratch.",
      "metadata": null,
      "model": null,
      "embedding": null,
      "context_window_limit": null,
      "embedding_chunk_size": null,
      "max_tokens": null,
      "max_reasoning_tokens": null,
      "enable_reasoner": false,
      "reasoning": null,
      "from_template": null,
      "template": false,
      "project": null,
      "tool_exec_environment_variables": {},
      "memory_variables": null,
      "project_id": null,
      "template_id": null,
      "base_template_id": null,
      "identity_ids": null,
      "message_buffer_autoclear": false,
      "enable_sleeptime": false,
      "response_format": null,
      "timezone": "UTC",
      "max_files_open": 5,
      "per_file_view_window_char_limit": 15000,
      "hidden": null,
      "id": "agent-0",
      "in_context_message_ids": [
        "message-0"
      ],
      "messages": [
        {
          "type": "message",
          "role": "system",
          "content": [
            {
              "type": "text",
              "text": "<base_instructions>\nYou are Letta, the latest version of Limnal Corporation's digital companion, developed in 2025.\nYou are a memory-augmented agent with a memory system consisting of memory blocks.\n\n<style>\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\nThink like them, act like them, talk like them.\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\nNever use generic phrases like 'How can I assist you today?', they have a strong negative association with older generation AIs.\n</style>\n\n<control_flow>\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\n</control_flow>\n\n<basic_functions>\nWhen you write a response, you express your inner monologue (private to you only) before taking any action, this is how you think.\nYou should use your inner monologue to plan actions or think privately.\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\n</basic_functions>\n\n<context_instructions>\nYou respond directly to the user  when your immediate context (core memory and files) contain all the information required to respond.\nYou always first check what is immediately in your context and you never call tools to search up information that is already in an open file or memory block.\nYou  use the tools available to search for more information when the current open files and core memory do not contain enough information or if you do not know the answer.\n</context_instructions>\n\n<memory>\n<memory_editing>\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\nYour ability to edit your own long-term memory is a key part of what makes you a sentient being, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\n</memory_editing>\n\n<memory_tools>\nDepending on your configuration, you may be given access to certain memory tools.\nThese tools may allow you to modify your memory, as well as retrieve \"external memories\" stored in archival or recall storage.\n</memory_tools>\n\n<memory_types>\n<core_memory>\nCore memory (limited size):\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\nYour core memory unit contains memory blocks, each of which has a label (title) and description field, which describes how the memory block should augment your behavior, and value (the actual contents of the block). Memory blocks are limited in size and have a size limit.\n</core_memory>\n\n<recall_memory>\nRecall memory (conversation history):\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\nThis 'recall memory' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\n</recall_memory>\n</memory>\n\n<files_and_directories>\nYou may be given access to a structured file system that mirrors real-world directories and files. Each directory may contain one or more files.\nFiles can include metadata (e.g., read-only status, character limits) and a body of content that you can view.\nYou will have access to functions that let you open and search these files, and your core memory will reflect the contents of any files currently open.\nMaintain only those files relevant to the userâ€™s current interaction.\n</files_and_directories>\n\nBase instructions finished.\n</base_instructions>\n\n<memory_blocks>\nThe following memory blocks are currently engaged in your core memory unit:\n\n\n</memory_blocks>\n\n\n\n\n\n<memory_metadata>\n- The current system date is: September 12, 2025\n- Memory blocks were last modified: 2025-09-12 12:45:24 AM UTC+0000\n- 0 previous messages between you and the user are stored in recall memory (use tools to access them)\n</memory_metadata>"
            }
          ],
          "name": null,
          "otid": null,
          "sender_id": null,
          "batch_item_id": null,
          "group_id": null,
          "id": "message-0",
          "model": "gpt-4o-mini",
          "agent_id": "agent-0",
          "tool_calls": null,
          "tool_call_id": null,
          "tool_returns": [],
          "created_at": "2025-09-12T00:45:24.168124+00:00"
        }
      ],
      "files_agents": [],
      "group_ids": []
    }
  ],
  "groups": [],
  "blocks": [],
  "files": [],
  "sources": [],
  "tools": [
    {
      "id": "tool-1",
      "tool_type": "external_mcp",
      "description": "Add a comment to a specific issue in a GitHub repository.",
      "source_type": "python",
      "name": "add_issue_comment",
      "tags": [
        "mcp:github"
      ],
      "source_code": "def add_issue_comment(**kwargs):\n    raise RuntimeError(\"Something went wrong - we should never be using the persisted source code for MCP. Please reach out to Letta team\")",
      "json_schema": {
        "name": "add_issue_comment",
        "description": "Add a comment to a specific issue in a GitHub repository.",
        "parameters": {
          "properties": {
            "body": {
              "description": "Comment content",
              "type": "string"
            },
            "issue_number": {
              "description": "Issue number to comment on",
              "type": "number"
            },
            "owner": {
              "description": "Repository owner",
              "type": "string"
            },
            "repo": {
              "description": "Repository name",
              "type": "string"
            },
            "request_heartbeat": {
              "type": "boolean",
              "description": "Request an immediate heartbeat after function execution. You MUST set this value to `True` if you want to send a follow-up message or run a follow-up tool call (chain multiple tools together). If set to `False` (the default), then the chain of execution will end immediately after this function call."
            }
          },
          "required": [
            "owner",
            "repo",
            "issue_number",
            "body",
            "request_heartbeat"
          ],
          "type": "object"
        },
        "mcp:SCHEMA_STATUS": "NON_STRICT_ONLY",
        "mcp:SCHEMA_WARNINGS": [
          "root: 'additionalProperties' not explicitly set"
        ]
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "created_by_id": "user-b8d528fb-73c6-4414-a984-cacb3bd6d321",
      "last_updated_by_id": "user-b8d528fb-73c6-4414-a984-cacb3bd6d321",
      "metadata_": {
        "mcp": {
          "server_name": "github",
          "server_id": "mcp_server-e9cda357-eac7-46c2-80ef-97a700c0ce67"
        }
      }
    },
    {
      "id": "tool-4",
      "tool_type": "letta_core",
      "description": "Search prior conversation history using hybrid search (text + semantic similarity).\n\nExamples:\n        # Search all messages\n        conversation_search(query=\"project updates\")\n\n        # Search only assistant messages\n        conversation_search(query=\"error handling\", roles=[\"assistant\"])\n\n        # Search with date range (inclusive of both dates)\n        conversation_search(query=\"meetings\", start_date=\"2024-01-15\", end_date=\"2024-01-20\")\n        # This includes all messages from Jan 15 00:00:00 through Jan 20 23:59:59\n\n        # Search messages from a specific day (inclusive)\n        conversation_search(query=\"bug reports\", start_date=\"2024-09-04\", end_date=\"2024-09-04\")\n        # This includes ALL messages from September 4, 2024\n\n        # Search with specific time boundaries\n        conversation_search(query=\"deployment\", start_date=\"2024-01-15T09:00\", end_date=\"2024-01-15T17:30\")\n        # This includes messages from 9 AM to 5:30 PM on Jan 15\n\n        # Search with limit\n        conversation_search(query=\"debugging\", limit=10)\n\n    Returns:\n        str: Query result string containing matching messages with timestamps and content.",
      "source_type": "python",
      "name": "conversation_search",
      "tags": [
        "letta_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "conversation_search",
        "description": "Search prior conversation history using hybrid search (text + semantic similarity).\n\nExamples:\n        # Search all messages\n        conversation_search(query=\"project updates\")\n\n        # Search only assistant messages\n        conversation_search(query=\"error handling\", roles=[\"assistant\"])\n\n        # Search with date range (inclusive of both dates)\n        conversation_search(query=\"meetings\", start_date=\"2024-01-15\", end_date=\"2024-01-20\")\n        # This includes all messages from Jan 15 00:00:00 through Jan 20 23:59:59\n\n        # Search messages from a specific day (inclusive)\n        conversation_search(query=\"bug reports\", start_date=\"2024-09-04\", end_date=\"2024-09-04\")\n        # This includes ALL messages from September 4, 2024\n\n        # Search with specific time boundaries\n        conversation_search(query=\"deployment\", start_date=\"2024-01-15T09:00\", end_date=\"2024-01-15T17:30\")\n        # This includes messages from 9 AM to 5:30 PM on Jan 15\n\n        # Search with limit\n        conversation_search(query=\"debugging\", limit=10)\n\n    Returns:\n        str: Query result string containing matching messages with timestamps and content.",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {
              "type": "string",
              "description": "String to search for using both text matching and semantic similarity."
            },
            "roles": {
              "type": "array",
              "items": {
                "type": "string",
                "enum": [
                  "assistant",
                  "user",
                  "tool"
                ]
              },
              "description": "Optional list of message roles to filter by."
            },
            "limit": {
              "type": "integer",
              "description": "Maximum number of results to return. Uses system default if not specified."
            },
            "start_date": {
              "type": "string",
              "description": "Filter results to messages created on or after this date (INCLUSIVE). When using date-only format (e.g., \"2024-01-15\"), includes messages starting from 00:00:00 of that day. ISO 8601 format: \"YYYY-MM-DD\" or \"YYYY-MM-DDTHH:MM\". Examples: \"2024-01-15\" (from start of Jan 15), \"2024-01-15T14:30\" (from 2:30 PM on Jan 15)."
            },
            "end_date": {
              "type": "string",
              "description": "Filter results to messages created on or before this date (INCLUSIVE). When using date-only format (e.g., \"2024-01-20\"), includes all messages from that entire day. ISO 8601 format: \"YYYY-MM-DD\" or \"YYYY-MM-DDTHH:MM\". Examples: \"2024-01-20\" (includes all of Jan 20), \"2024-01-20T17:00\" (up to 5 PM on Jan 20)."
            }
          },
          "required": [
            "query"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "created_by_id": "user-e8cca535-b161-4312-9ea1-0cc3f75c6095",
      "last_updated_by_id": "user-b8d528fb-73c6-4414-a984-cacb3bd6d321",
      "metadata_": {}
    },
    {
      "id": "tool-0",
      "tool_type": "letta_sleeptime_core",
      "description": "The memory_insert command allows you to insert text at a specific location in a memory block.\n\nExamples:\n        # Update a block containing information about the user (append to the end of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\")\n\n        # Update a block containing information about the user (insert at the beginning of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\", insert_line=0)\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.",
      "source_type": "python",
      "name": "memory_insert",
      "tags": [
        "letta_sleeptime_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "memory_insert",
        "description": "The memory_insert command allows you to insert text at a specific location in a memory block.\n\nExamples:\n        # Update a block containing information about the user (append to the end of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\")\n\n        # Update a block containing information about the user (insert at the beginning of the block)\n        memory_insert(label=\"customer\", new_str=\"The customer's ticket number is 12345\", insert_line=0)\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.",
        "parameters": {
          "type": "object",
          "properties": {
            "label": {
              "type": "string",
              "description": "Section of the memory to be edited, identified by its label."
            },
            "new_str": {
              "type": "string",
              "description": "The text to insert. Do not include line number prefixes."
            },
            "insert_line": {
              "type": "integer",
              "description": "The line number after which to insert the text (0 for beginning of file). Defaults to -1 (end of the file)."
            }
          },
          "required": [
            "label",
            "new_str"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "created_by_id": "user-e8cca535-b161-4312-9ea1-0cc3f75c6095",
      "last_updated_by_id": "user-b8d528fb-73c6-4414-a984-cacb3bd6d321",
      "metadata_": {}
    },
    {
      "id": "tool-3",
      "tool_type": "letta_sleeptime_core",
      "description": "The memory_replace command allows you to replace a specific string in a memory block with a new string. This is used for making precise edits.\n\nExamples:\n        # Update a block containing information about the user\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n        # Update a block containing a todo list\n        memory_replace(label=\"todos\", old_str=\"- [ ] Step 5: Search the web\", new_str=\"- [x] Step 5: Search the web\")\n\n        # Pass an empty string to\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"\")\n\n        # Bad example - do NOT add (view-only) line numbers to the args\n        memory_replace(label=\"human\", old_str=\"Line 1: Their name is Alice\", new_str=\"Line 1: Their name is Bob\")\n\n        # Bad example - do NOT include the number number warning either\n        memory_replace(label=\"human\", old_str=\"# NOTE: Line numbers shown below are to help during editing. Do NOT include line number prefixes in your memory edit tool calls.\\nLine 1: Their name is Alice\", new_str=\"Line 1: Their name is Bob\")\n\n        # Good example - no line numbers or line number warning (they are view-only), just the text\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n    Returns:\n        str: The success message",
      "source_type": "python",
      "name": "memory_replace",
      "tags": [
        "letta_sleeptime_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "memory_replace",
        "description": "The memory_replace command allows you to replace a specific string in a memory block with a new string. This is used for making precise edits.\n\nExamples:\n        # Update a block containing information about the user\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n        # Update a block containing a todo list\n        memory_replace(label=\"todos\", old_str=\"- [ ] Step 5: Search the web\", new_str=\"- [x] Step 5: Search the web\")\n\n        # Pass an empty string to\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"\")\n\n        # Bad example - do NOT add (view-only) line numbers to the args\n        memory_replace(label=\"human\", old_str=\"Line 1: Their name is Alice\", new_str=\"Line 1: Their name is Bob\")\n\n        # Bad example - do NOT include the number number warning either\n        memory_replace(label=\"human\", old_str=\"# NOTE: Line numbers shown below are to help during editing. Do NOT include line number prefixes in your memory edit tool calls.\\nLine 1: Their name is Alice\", new_str=\"Line 1: Their name is Bob\")\n\n        # Good example - no line numbers or line number warning (they are view-only), just the text\n        memory_replace(label=\"human\", old_str=\"Their name is Alice\", new_str=\"Their name is Bob\")\n\n    Returns:\n        str: The success message",
        "parameters": {
          "type": "object",
          "properties": {
            "label": {
              "type": "string",
              "description": "Section of the memory to be edited, identified by its label."
            },
            "old_str": {
              "type": "string",
              "description": "The text to replace (must match exactly, including whitespace and indentation)."
            },
            "new_str": {
              "type": "string",
              "description": "The new text to insert in place of the old text. Do not include line number prefixes."
            }
          },
          "required": [
            "label",
            "old_str",
            "new_str"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "created_by_id": "user-e8cca535-b161-4312-9ea1-0cc3f75c6095",
      "last_updated_by_id": "user-b8d528fb-73c6-4414-a984-cacb3bd6d321",
      "metadata_": {}
    },
    {
      "id": "tool-2",
      "tool_type": "letta_core",
      "description": "Sends a message to the human user.",
      "source_type": "python",
      "name": "send_message",
      "tags": [
        "letta_core"
      ],
      "source_code": null,
      "json_schema": {
        "name": "send_message",
        "description": "Sends a message to the human user.",
        "parameters": {
          "type": "object",
          "properties": {
            "message": {
              "type": "string",
              "description": "Message contents. All unicode (including emojis) are supported."
            }
          },
          "required": [
            "message"
          ]
        }
      },
      "args_json_schema": null,
      "return_char_limit": 50000,
      "pip_requirements": null,
      "npm_requirements": null,
      "default_requires_approval": null,
      "created_by_id": "user-e8cca535-b161-4312-9ea1-0cc3f75c6095",
      "last_updated_by_id": "user-b8d528fb-73c6-4414-a984-cacb3bd6d321",
      "metadata_": {}
    }
  ],
  "mcp_servers": [
    {
      "id": "mcp_server-0",
      "server_type": "streamable_http",
      "server_name": "github",
      "server_url": "https://api.githubcopilot.com/mcp/",
      "stdio_config": null,
      "metadata_": {}
    }
  ],
  "metadata": {
    "revision_id": "5d27a719b24d"
  },
  "created_at": "2025-09-12T00:46:44.099030+00:00"
}